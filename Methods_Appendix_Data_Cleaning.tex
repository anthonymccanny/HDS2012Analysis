\documentclass{article}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage{hyperref}

% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Code listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{Extended Methods Appendix: Data Cleaning Procedures for Housing Discrimination Study (2012) Analysis}
\author{}
\date{}

\begin{document}

\maketitle

\section{Overview}

This appendix provides comprehensive documentation of the data cleaning procedures applied to the Housing Discrimination Study (HDS) 2012 dataset. The cleaning process transforms raw SAS data files into analysis-ready datasets suitable for econometric analysis.
\section{Data Sources and Initial Processing}

\subsection{Raw Data Structure}

The HDS 2012 data consists of six primary SAS files in \texttt{.sas7bdat} format. Each file serves a distinct purpose in documenting the field experiment:

The \textbf{assignment} file links individual testers to specific test assignments and contains release status indicators. The \textbf{taf} (Test Assignment Form) file provides test-level characteristics and parameters. The \textbf{sales} file contains detailed records of each sales test visit, including appointment times and outcomes. The \textbf{tester\_censored} file provides tester demographic and background information with personally identifiable information removed. The \textbf{rhgeo} file contains geocoded data for recommended homes shown during each test. Finally, the \textbf{rechomes} file provides additional information about recommended properties.

\subsection{Sales Test Filtering}

The HDS 2012 study encompasses both rental and sales discrimination tests. For this analysis, we focus exclusively on sales tests, which are identified through control numbers containing the pattern ``-S[A-Z]-''. This pattern corresponds to three test types: SA for Asian testers, SB for Black testers, and SH for Hispanic testers.

The filtering process is applied consistently across all datasets:

\begin{lstlisting}[language=R]
assignment <- assignment_raw %>%
  filter(grepl("-S[A-Z]-", CONTROL)) %>%
  filter(RELEASE == "1" & !is.na(TESTERID))

taf <- taf_raw %>%
  filter(grepl("-S[A-Z]-", CONTROL))
\end{lstlisting}

For the assignment file, we apply an additional filter requiring \texttt{RELEASE = "1"}, which indicates completed and valid test assignments. This ensures our analysis includes only tests that were successfully executed in the field.

\section{Date and Time Processing}

\subsection{The Challenge of Date Format Heterogeneity}

One of the most significant data cleaning challenges involves parsing appointment dates (\texttt{SAPPTD}) and birth dates (\texttt{TDOB}), which exhibit remarkable format diversity. Our analysis identified over 15 distinct date format patterns in the raw data, reflecting the challenges of manual data entry across many distinct testers.

The date formats encountered include standard formats with various separators (``MM/DD/YYYY'', ``MM-DD-YYYY'', ``MM.DD.YYYY''), formats without separators (``MMDDYYYY'', ``MMDDYY''), ambiguous formats that could represent multiple date interpretations (``MM/YY'', ``DD/MM'', ``MMDD/YY''), and numerous data entry errors including impossible dates (``6/31/12''), leap year errors (``02/29/2011''), and clear typographical errors (``08/18/7977'', ``01/08/1064'').

\subsection{Date Parsing Algorithm}

To address this heterogeneity, we developed a comprehensive parsing function that employs multiple strategies. First, the function standardizes all separators (dashes, dots, spaces) to forward slashes and removes extraneous characters. Second, it maintains a lookup table for known invalid dates that require manual correction. Third, it uses regular expression pattern matching to identify date formats, testing patterns in order of specificity. Fourth, it handles century ambiguity through context-aware logic: for birth dates, two-digit years are interpreted as 19XX (reflecting that testers were born in the 1900s), while for appointment dates, two-digit years are interpreted as 20XX (reflecting that tests occurred in 2011-2012).

The date parsing function handles ambiguous two-number date formats, such as "MM/YY" or "DD/MM" by first checking if the first number is greater than 12, which would make it invalid as a month. In this case, it assumes the date is in a "DD/MM" format and the year is inferred later in the data cleaning process.

If both numbers could plausibly represent either a month (1–12) or a day (1–31), the function uses additional context to resolve the ambiguity. For birth dates, it first attempts to interpret the date as "MM/YY" and calculates the resulting age as of 2011. If this age is reasonable—between 18 and 90 years old—it accepts this interpretation. For appointment dates, the function checks if the second number is 11 or 12, which likely indicates the years 2011 or 2012. If so, it interprets the date as "MM/YY". If not, it defaults to interpreting the date as "DD/MM" with the placeholder year 2000, again to be corrected later.

Finally, if the first number is too large to represent a valid month or day (greater than 31), the function considers the possibility of a "YY/MM" format. For birth dates, it again validates the resulting age, ensuring it falls within a plausible range (18–90 years old in 2011). For appointment dates, it specifically checks if the first number is 11 or 12, indicating the years 2011 or 2012, and interprets the date accordingly.

If none of these logical steps yield a valid interpretation, the function returns a missing value (NA) and issues a warning. This structured and context-aware approach ensures ambiguous date formats are handled consistently, transparently, and logically, with clear reasoning guiding each decision.

\begin{lstlisting}[language=R]
# Example of ambiguous format handling
"^\\d{1,2}/\\d{2}$" = function(x) {
  parts <- strsplit(x, "/")[[1]]
  first_num <- as.numeric(parts[1])
  second_num <- as.numeric(parts[2])
  
  could_be_month <- function(n) !is.na(n) && n >= 1 && n <= 12
  could_be_day <- function(n) !is.na(n) && n >= 1 && n <= 31
  
  # If first number too large for month, likely DD/MM format
  if (!could_be_month(first_num) && could_be_day(first_num) && 
      could_be_month(second_num)) {
    if (!is_birth_date) {
      return(as.Date(sprintf("%02d/%02d/2011", second_num, first_num), 
                     format = "%m/%d/%Y"))
    }
  }
  # Additional logic for other scenarios...
}
\end{lstlisting}

\subsection{Systematic Date Correction}

Some parsed appointment dates fall outside the study period (July 2011 through October 2012), indicating data entry errors where incorrect years or months were recorded. Other times, years are missing. Our correction algorithm addresses these systematically by leveraging the fact that multiple appointments within the same test (identified by \texttt{CONTROL} number) should occur within a reasonable timeframe.

The correction process identifies all dates outside the valid study period, then for each invalid date, searches for other appointments with the same \texttt{CONTROL} number that have valid dates. When valid reference dates exist, the algorithm preserves the day information from the invalid date but adopts the year from the valid appointments. If the corrected year matches the original (suggesting a month error rather than a year error), the algorithm adjusts the month instead. When no valid reference date exists for a given \texttt{CONTROL}, the appointment date is set to missing rather than imputed.

\subsection{Time Parsing and Correction}

Appointment times (\texttt{SBEGH}) present similar challenges, with formats including military time (``0315'', ``1430''), three-digit formats (``357'' interpreted as 3:57), various separated formats (``3:15'', ``3;15'', ``3.15''), and invalid entries (``N/A'', non-numeric values). The complexity is compounded by AM/PM indicators (\texttt{SBEGAM}) that are sometimes missing or clearly incorrect.

Our time parsing function handles these variations systematically. For missing AM/PM indicators, the function infers the most likely period based on the hour: times between 9 and 12 are likely morning appointments, while times between 1 and 8 are likely afternoon or evening appointments. The function also corrects obviously incorrect indicators, such as times between 1:00 and 5:00 marked as AM (changed to PM) or times between 10:00 and 11:00 marked as PM (changed to AM).

A particular challenge involves appointments recorded at exactly midnight (00:00:00), which is implausible for real estate showings. Our analysis revealed that these entries typically represent data entry errors where time information was missing. To address this, we identify all midnight appointments and check whether other appointments exist on the same day for the same \texttt{CONTROL}. If multiple appointments exist on the same day, the midnight time is set to missing, indicating a time entry error. Midnight times are preserved only when they represent the sole appointment for that day, though even these cases likely represent missing data. This is done because appointment times mainly enter the analysis as covariates for visit order, and inaccurate timing does not impact visit order when no other appointments exist for that day.

\section{Data Aggregation and Variable Construction}

\subsection{Visit Order Determination}

The experimental design allowed testers to make multiple visits to the same real estate agent. To properly analyze outcomes, we must establish the chronological order of these visits. Our approach groups records by \texttt{CONTROL} number, sorts by the combined appointment date and time, and assigns visit order using sequential numbering. Records with missing datetime information are placed last in the sequence.

\subsection{Creating the Analysis Dataset}

The final sales dataset aggregates information across multiple visits for each tester-control combination. This aggregation preserves both visit-specific information from the first interaction and cumulative information across all visits. The aggregation process creates several categories of variables:

First visit metrics capture the initial interaction between tester and agent, including \texttt{SBEGAM\_FIRST} (appointment time AM/PM indicator), \texttt{STOTUNIT\_FIRST} (number of units shown), and \texttt{SAVLBAD\_FIRST} (indicator for whether the tester was told the house in the ad was available). Aggregate metrics summarize the entire test sequence, including \texttt{num\_visits} (total number of visits), \texttt{STOTUNIT\_TOTAL} (total units shown across all visits), and \texttt{SAVLBAD\_ANY} (indicator for whether the original ad house was indicated to be available in any visit). Temporal metrics provide timing information, including \texttt{first\_visit\_datetime} and \texttt{was\_first\_visitor} (indicator for whether this tester was the first to visit the agent in this test).

The aggregation also includes extraction of race categories from the \texttt{RACEID} variable. This extraction uses regular expressions to identify the numeric race code embedded at the end of the race identifier string. Additionally, the process includes special handling for infinite values that can arise when calculating minimum datetimes across empty sets, converting these to proper missing values.

\begin{lstlisting}[language=R]
sales_final <- sales_with_time %>%
  group_by(CONTROL) %>%
  arrange(appointment_datetime) %>%
  mutate(visit_order = row_number()) %>%
  ungroup() %>%
  group_by(CONTROL, TESTERID) %>%
  summarise(
    RACEID = first(RACEID),
    SBEGAM_FIRST = first(SBEGAM[which.min(visit_order)]),
    STOTUNIT_FIRST = first(STOTUNIT[which.min(visit_order)]),
    SAVLBAD_FIRST = first(SAVLBAD_BINARY[which.min(visit_order)]),
    first_visit_datetime = min(appointment_datetime, na.rm = TRUE),
    num_visits = n(),
    STOTUNIT_TOTAL = sum(STOTUNIT, na.rm = TRUE),
    SAVLBAD_ANY = as.numeric(any(SAVLBAD_BINARY == 1, na.rm = TRUE)),
    was_first_visitor = any(visit_order == 1, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  mutate(
    RACE = as.numeric(str_extract(as.character(RACEID), "\\d$")),
    first_visit_datetime = if_else(is.infinite(first_visit_datetime), 
                                 as.POSIXct(NA), first_visit_datetime)
  )
\end{lstlisting}

\textbf{Note:} The only occurrence where \texttt{appointment\_datetime} is missing while \texttt{appointment\_date} is present represents a data entry correction. Specifically, this case occurs when an initial record contained an invalid time entry (typically 00:00), prompting data entry personnel to create a subsequent record with the corrected time information. The original record with the invalid time is retained in the dataset but lacks a valid \texttt{appointment\_datetime}. This incomplete record can be and is safely excluded from analysis as it represents a superseded data entry rather than genuine missing information.

\section{Tester Demographics Processing}

Tester birth dates undergo the same parsing process as appointment dates, with the \texttt{is\_birth\_date} parameter set to TRUE to apply appropriate century logic. Ages are calculated as of January 1, 2012, representing the approximate midpoint of the study period. The calculation uses precise day-based computation (days divided by 365.25 to account for leap years), rounds to the nearest whole year, and excludes implausible ages (under 18 years old) by setting them to missing.

The final step merges the aggregated sales data with tester demographics using left joins on \texttt{TESTERID}, preserving all sales records even when tester demographic information may be incomplete.

\section{Output Files}

The cleaning process generates two primary output files. First, \texttt{sales\_cleaned.csv} contains the aggregated sales data with one row per tester-control combination, suitable for most analyses of discrimination outcomes. Second, \texttt{sales\_and\_tester\_merged.csv} includes the sales data merged with tester demographics, enabling analyses that incorporate tester characteristics.

\section{Recommended Homes Data Integration}

\subsection{Data Deduplication and Quality Control}

\subsubsection{Rechomes Data Deduplication}

The recommended homes (\texttt{rechomes}) dataset contains records of properties shown to testers during their visits. Initial inspection revealed the presence of duplicate records that needed systematic resolution before analysis. The deduplication process addresses duplicates arising from multiple sources, including data entry errors and repeated records of the same property.

Our deduplication strategy identifies duplicate records based on the complete address specification: \texttt{CONTROL}, \texttt{TESTERID}, \texttt{HSITEAD} (street address), \texttt{HSITETYP} (site type), \texttt{HUNITNO} (unit number), \texttt{HCITY}, \texttt{HSTATE}, and \texttt{HZIP}. When duplicates are identified, we retain the first occurrence and remove subsequent records. This approach assumes that the first entry represents the original data capture, with subsequent duplicates arising from data processing or entry errors.

The deduplication process reduced the rechomes dataset from 24,859 initial rows to 24,679 unique property-tester combinations, removing 180 duplicate records (0.72\% of the initial dataset). This modest reduction suggests generally good data quality with limited duplication issues.

\subsubsection{Rhgeo Data Processing and Cleaning}

The \texttt{rhgeo} dataset provides geocoded information for recommended homes, enabling geographic analysis of property recommendations. The processing of this dataset involves three sequential steps to ensure data quality and consistency.

First, we remove exact duplicate records that may have arisen during data processing or merging. This step reduced the dataset from 16,643 initial rows to 16,358 rows, removing 285 exact duplicates (1.71\% of records). These duplicates likely arose from data processing errors rather than genuine repeated property showings.

Second, we identify and remove records with malformed or missing address information that would prevent meaningful geographic analysis. The filtering criteria exclude records where the street address field contains non-informative values such as ``unknown'', ``did not provide'', ``MODEL HOME'', or variations thereof. We also exclude records with effectively empty addresses (containing only whitespace, dashes, or zeros) and addresses too short to be meaningful (three characters or fewer after trimming). This filtering removed 15 additional records with problematic addresses including: ``255'', ``Covington/Model Home'', ``did not provide'', ``MODEL HOME'', ``Model home/ Castleberry model'', ``Model home/Brookland model'', ``Model home/Chelsea model'', ``none'', ``Sananah/Model Home'', ``unknown'', and ``unknown (Rangler house)''.

Third, we perform address-based deduplication similar to that applied to the rechomes data, retaining the first occurrence when multiple records share identical address components. This final step removed only one additional duplicate, yielding 16,342 unique geocoded property records.

\subsubsection{Rechomes-Rhgeo Integration Analysis}

Our merge analysis reveals an asymmetric relationship between these datasets. All 16,342 records in the deduplicated \texttt{rhgeo} dataset have corresponding entries in \texttt{rechomes}, indicating complete coverage of geocoded properties. However, 8,337 records in \texttt{rechomes} (33.78\%) lack corresponding geocoded information in \texttt{rhgeo}. 

Notably, the proportion of rechomes records with geocoding (66.22\%) represents almost exactly two-thirds of the dataset, suggesting a systematic sampling or processing decision by the original data providers rather than geocoding failures. Investigation of the addresses that were and were not geocoded reveals no apparent systematic differences in address quality or format between the two groups. This pattern indicates that the original researchers likely geocoded a representative subset of properties rather than attempting comprehensive geocoding, possibly due to resource constraints or analytical requirements that did not necessitate complete geographic coverage.

The merged \texttt{rechomes\_rhgeo} dataset retains all 24,679 records from the deduplicated \texttt{rechomes} data, with geocoded information available for the two-thirds subset that was processed in the original study. This approach preserves maximum information while clearly identifying which properties have geographic data available for spatial analyses.

\subsection{Analytical Sample Definition}

Before merging recommended homes data, we first establish our analytical sample by applying key filtering criteria to ensure data quality and experimental validity. The analytical sample is constructed through a two-stage filtering process that addresses both outcome availability and experimental design requirements.

First, we exclude observations with missing outcome variables. Specifically, we require non-missing values for both \texttt{STOTUNIT\_TOTAL} (total units shown across all visits) and \texttt{SAVLBAD\_ANY} (whether the advertised property was available in any visit). These variables represent our primary measures of treatment by real estate agents and are essential for discrimination analysis.

Second, we enforce the paired testing experimental design by retaining only tests (\texttt{CONTROL} numbers) that include exactly two testers. This restriction ensures that each test includes one minority tester and one white tester, enabling direct pairwise comparisons that form the foundation of the experimental methodology.

\begin{lstlisting}[language=R]
analytic_sales_tester <- sales_and_tester %>%
  filter(!is.na(STOTUNIT_TOTAL), !is.na(SAVLBAD_ANY)) %>%
  group_by(CONTROL) %>%
  filter(n() == 2) %>%  # Exactly 2 testers per control
  ungroup()
\end{lstlisting}

This filtering process demonstrates the data attrition inherent in creating a high-quality analytical dataset. From an initial 12,030 observations in the merged sales and tester data, requiring non-missing outcomes reduces the sample to 8,662 observations. Subsequently, requiring exactly two testers per control yields 7,302 observations representing 3,651 unique test pairs. The loss of observations primarily reflects incomplete data collection for some tests and instances where one tester in a pair failed to complete their assignment or had their data excluded during quality control.

\subsection{Recommended Homes Data Merging}

The merging process employs a left join on \texttt{CONTROL} and \texttt{TESTERID}, preserving all observations from the analytical sample while adding available property information from the merged \texttt{rechomes\_rhgeo} dataset. This approach acknowledges that recommended homes data may not exist for all testers, either because they were shown no properties or because property data was not successfully collected or recorded.

\begin{lstlisting}[language=R]
sales_tester_rechomes <- analytic_sales_tester %>%
  left_join(rechomes_rhgeo, by = c("CONTROL", "TESTERID")) %>%
  mutate(has_rechomes_data = rowSums(!is.na(select(., HSITEAD, HZIP))) > 0)
\end{lstlisting}

\subsubsection{Initial Merge Statistics}

The merge with recommended homes data reveals several important patterns in data completeness. The analytical sample contains 3,651 unique test pairs (7,302 individual tester observations). Among these, 1,012 test pairs (27.7\%) have at least one tester for whom recommended homes data is missing. 

Examining the relationship between recommendations and data availability, we find that 2,717 test pairs (74.4\%) involve cases where both testers received at least one property recommendation (i.e., neither tester has \texttt{STOTUNIT\_TOTAL = 0}). Despite agents recommending properties, 158 individual observations (2.2\% of the sample) show positive values for \texttt{STOTUNIT\_TOTAL} but lack the corresponding detailed property information in the rechomes dataset. At the test-pair level, 143 controls (3.9\%) have at least one tester who was shown properties but for whom detailed property data is missing.

\subsubsection{Data Recovery Through Donor Imputation}

A critical subset of the missing data represents cases where testers reported being shown only the advertised home (\texttt{STOTUNIT\_TOTAL = 1} and \texttt{SAVLBAD\_ANY = 1}) but lack corresponding property details. Investigation revealed that in many such cases, the partnered tester within the same test had recorded information about the advertised property.

Our donor imputation approach leverages this within-test information:
\begin{enumerate}
\item Identify observations with \texttt{STOTUNIT\_TOTAL = 1}, \texttt{SAVLBAD\_ANY = 1}, and missing rechomes data
\item For each such observation, search for records within the same \texttt{CONTROL} where \texttt{HADHOME = 1} (indicating the advertised home was recorded)
\item When a donor record is found, copy the property information (excluding \texttt{TESTERID}) to the observation with missing data
\end{enumerate}

\begin{lstlisting}[language=R]
# Find potential donor rows (same CONTROL, HADHOME == 1)
donor_rows <- rechomes %>%
  filter(CONTROL %in% missing_data$CONTROL, HADHOME == 1) %>%
  select(-TESTERID) %>%
  group_by(CONTROL) %>%
  slice(1) %>%  # Take first matching row per CONTROL
  ungroup()

# Copy donor information to observations with missing data
for (i in which(needs_rechomes_fix)) {
  current_control <- .$CONTROL[i]
  donor_match <- donor_rows[donor_rows$CONTROL == current_control, ]
  
  if (nrow(donor_match) > 0) {
    rechomes_cols <- names(donor_match)[!names(donor_match) %in% c("CONTROL")]
    for (col in rechomes_cols) {
      .[[col]][i] <- donor_match[[col]][1]
    }
    .$has_rechomes_data[i] <- TRUE
    rows_fixed <- rows_fixed + 1
  }
}
\end{lstlisting}

This recovery process successfully restored property information for 43 of 66 eligible observations (65.2\% recovery rate). The 23 unrecovered observations represent cases where no donor information was available within the test, likely because neither tester received detailed information about the advertised property. After this data recovery, the number of controls with at least one tester having recommendations but missing rechomes data decreased from 143 to 101.

\subsection{Final Analytical Dataset Construction}

To ensure the integrity of our paired testing analysis, we apply a final filtering step that retains only complete test pairs where both testers have recommended homes data available. This approach is conservative but necessary for analyses that require property-level comparisons between minority and white testers within the same test.

The filtering process requires that observations have at least some recommended homes data and that each \texttt{CONTROL} number includes exactly two testers with such data. This dual requirement ensures that every observation in the final dataset has a valid comparison partner within the same test.

\begin{lstlisting}[language=R]
sales_tester_rechomes <- sales_tester_rechomes %>%
  filter(has_rechomes_data) %>%
  group_by(CONTROL) %>%
  filter(n_distinct(TESTERID) == 2) %>%
  ungroup()
\end{lstlisting}

This final filtering step yields our complete analytical dataset of 18,990 property records across 2,714 unique test pairs. The reduction from 3,651 to 2,714 test pairs represents a loss of 937 pairs (25.7\%), primarily driven by the requirement that both testers in each pair must have successfully recorded property information. While this attrition is substantial, it ensures that every test in our final sample enables direct within-pair comparisons of the properties shown to minority versus white testers.


The resulting dataset contains several categories of variables for analysis. Tester-level variables include demographics (\texttt{TAGE}, \texttt{RACE}), test outcomes (\texttt{STOTUNIT\_TOTAL}, \texttt{SAVLBAD\_ANY}), and visit characteristics (\texttt{first\_visit\_datetime}, \texttt{num\_visits}). Property-level variables from the recommended homes data include housing characteristics, location information, and pricing data, all prefixed with ``H''. Test-level identifiers include \texttt{CONTROL} (linking testers within tests) and \texttt{TESTERID} (identifying individual testers).

\subsection{Data Quality Implications}

The substantial sample reduction during recommended homes integration raises important considerations for analysis interpretation. The final analytical sample represents approximately 73.5\% of valid test pairs, suggesting that results should be interpreted as applying to tests where property recommendation data is available rather than the entire population of housing discrimination tests.

However, this sample restriction is methodologically sound for several reasons. First, analyses of discriminatory differences in property recommendations require property data for both testers in each pair. Second, the majority of sample loss occurs randomly due to data collection challenges rather than systematic patterns that might bias results. Third, the remaining sample is sufficiently large (2,684 test pairs) to provide adequate statistical power for detecting meaningful differences in treatment.

\end{document}